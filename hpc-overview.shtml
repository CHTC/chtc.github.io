
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">

<html>
<head>
<script type="text/javascript">

  function resizeIframe(iframe) {
    var newheight;
    var newwidth;

    if(document.getElementById){
        newwidth=iframe.contentWindow.document.body.scrollWidth;
        newheight=iframe.contentWindow.document.body.scrollHeight;
    }

    iframe.height= (newheight) + "px";
    iframe.width= (newwidth) + "px";
  }
</script>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>HPC Cluster Overview</title>

<link href="/web.css" rel="stylesheet" type="text/css">
</head>
<body leftmargin="0" topmargin="0" marginwidth="0" marginheight="0">

<table width="100%" border="0" cellspacing="0" cellpadding="0">
<tr>
   <td width="1%" align="left" class="bgred" valign="top">
      <div align="center">
      &nbsp;<br>
      <a href="http://www.wisc.edu">

      <img src="/images/uw-sm-red.png" width="125" height="46" border="0"> </a>
       </div>

	<ul class="sidebar">
	<li><a href="/">CHTC Home</a></li>
	
	<li>About
		<ul>
		<li><a href="/approach.shtml">Our Approach</a></li>
<!--comment		<li><a href="/resources.shtml">Our Resources</a></li>   -->
		<li><a href="/projects.shtml">Our Customers</a></li>
		<li><a href="/people.shtml">Our Staff</a></li>
		<li><a href="/jobs.shtml">Our Open Positions</a></li>
		</ul>
	</li>
	
	<li>How To's
		<ul>
		<li><a href="/get-started.shtml">Get Started</a></li>
		<li><a href="/get-help.shtml">Get Help</a></li>
        	<li><a href="/guides.shtml">All User Guides</a></li>
                <li><a href="/use-submit-node.shtml">Use an HTC Submit Server</a></li>
		<li><a href="/helloworld.shtml">Run Your First HTC Jobs</a></li>
		<li><a href="/howto_overview.shtml">HTC for MatLab, Python or R</a></li>
		<li><a href="/hpc-overview.shtml">Use the HPC Cluster</a></li>
		<li><a href="/cite-chtc.shtml">Cite CHTC Resources</a></li>
		</ul>
	</li>
	<li><a href="/user-news.shtml">User News</a></li>
	<li>Other Resources
		<ul>
		<!--<li><a href="http://monitor.chtc.wisc.edu/uw_condor_usage/usage1.shtml">Pool Usage Reports</a></li>-->
<!--comment		<li><a href="/opmetrics.shtml">Operational Metrics</a></li>  -->
		<li><a href="/gpu-lab.shtml">CHTC GPU Lab</a></li>
		<li><a href="http://research.cs.wisc.edu/htcondor/">HTCondor Project</a></li>
		<li><a href="http://www.neos-server.org/">NEOS Optimization Service</a></li>
		<li><a href="http://www.opensciencegrid.org/">Open Science Grid</a></li>
		<li><a href="http://wid.wisc.edu/">WID (Wisconsin Institute for Discovery)</a></li>
		</ul>
	</li>
	</ul>

	</td>

<td width="97%" valign="top"> 


	<TABLE SUMMARY="Navigation layout" BORDER="0" WIDTH="100%" CELLSPACING="0" CELLPADDING="0">
<TR ALIGN="left"> 
  <TD WIDTH="100%" height="400" Align="left" VALIGN="top"> 
    <A  name="body"></a> 
	<div id = "main">

<!-- Top of Page Body -->
<table BORDER="0" WIDTH="100%">
<!-- <h1 align=left valign=center>The Center for High Throughput Computing</h1> -->

<div id="osg_power"> <span style="height: 30px">Powered by:</span><br /><a href="http://opensciencegrid.org"><img alt="Open Science Grid" src="/images/Open_Science_Grid_Consortium(Logo).jpg" width="123" height="70" /></a></div>

<div id="hours"> <span style="height: 30px">
	<a href="https://twitter.com/CHTC_UW" style="color:white;"><center><img alt="Twitter" src="/images/twitter.png" width="50" height="50"></center></a> 
	<center><a href="https://github.com/CHTC/chtc-website-source" style="color:white;"><img alt="GitHub" src="/images/GitHub_Logo_White.png" width="92" height="38"></center></a></span></div>

<div id="hours"> <span style="height: 35px">
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<a href="http://chtc.cs.wisc.edu/get-help.shtml" style="color:white;">
<center><b>Office Hours!</b>
<p style="margin-bottom: 1px;"></p>
Tues/Thurs, 3-5pm
Click for details
</center></a></span>
<p style="margin-bottom: 1px;"></p>
<center><a href="http://chtc.cs.wisc.edu/sign-in.shtml" style="color:white;">Sign-In Here</a></center>
</div>
	
<div style="margin-top:1em;"><a href="http://chtc.cs.wisc.edu/"><img alt="Center for High Throughput Computing" src="/images/CHTC-logo.png" width="500" height="90"></a></div>


<link rel="stylesheet" type="text/css" href="/bootstrap.css" />

<h1>Additional HPC Guides</h1>
<br>
<div class="card-deck">
	<div class="card border-secondary h-100 text-center">  
		<a href="connecting.shtml"><li class="list-group-item list-group-item-action list-group-item-dark h-100" style="width: auto; height: auto !important;">Connecting to CHTC</li></a>
	</div>
	<div class="card border-secondary h-100 text-center">  
		<a href="hpc-overview.shtml"><li class="list-group-item list-group-item-action list-group-item-dark h-100" style="width: auto; height: auto !important;">HPC Overview</li></a>
	</div>
	<div class="card border-secondary h-100 text-center">  
		<a href="hpc-job-submission.shtml"><li class="list-group-item list-group-item-action list-group-item-dark h-100" style="width: auto; height: auto !important;">HPC Job Submission</li></a>
	</div>
	<div class="card border-secondary h-100 text-center">  
		<a href="hpc-software.shtml"><li class="list-group-item list-group-item-action list-group-item-dark h-100" style="width: auto; height: auto !important;">HPC Software</li></a>
	</div>
</div>
<hr>
<h1>HPC Cluster Overview</h1>


<h1 id="content">Content</h1>

<ol>
  <li><a href="#high-performance-computing-at-chtc">High-Performance Computing at CHTC</a></li>
  <li><a href="#hpc-user-policies">HPC User Policies</a></li>
  <li><a href="#hpc-hardware-and-configuration">HPC Hardware and Configuration</a>
    <ul>
      <li><a href="#partitions">Partitions</a></li>
      <li><a href="#operating-system-and-software">Operating System and Software</a></li>
    </ul>
  </li>
  <li><a href="#data-storage-and-management">Data Storage and Management</a>
    <ul>
      <li><a href="#tools-for-managing-home-and-software-space">Tools for managing home and software space</a></li>
    </ul>
  </li>
</ol>

<h1 id="high-performance-computing-at-chtc">High-Performance Computing at CHTC</h1>

<p>The CHTC high-performance computing (HPC) cluster provides dedicated support for large, 
singular computations that use specialized software (i.e. MPI) to achieve internal 
parallelization of work across multiple servers of dozens to hundreds of cores.</p>

<p><strong><em>Is high-performance computing right for me?</em></strong> Only computational work that 
fits that above description is appropriate for the HPC Cluster. Computational 
work that can complete on a single node in less than a few days will be 
best supported by our larger high-throughput computing (HTC) system (which also
includes specialized hardware for extreme memory, GPUs, and other cases). For more 
information, please see <a href="/approach">Our Approach</a>.</p>

<p>To get access to the HPC Cluster, please complete our
<a href="/form">New User Consultation Form</a>. After your request is received, 
a Research Computing Facilitator will follow up to discuss the computational needs 
of your research and connect you with computing 
resources (including non-CHTC services) that best fit your needs.</p>

<h1 id="hpc-user-policies">HPC User Policies</h1>

<p>Below is a list of policies that apply to all HPC users.</p>

<p><strong>1. Minimize Work on the Login Nodes</strong> <br />
The HPC Cluster login nodes have 
limited computing resources that are occupied with running Slurm and managing job submission, 
and are not suitable for testing your research software.</p>

<p>Users may run basic data management commands (like <code>tar</code>, <code>cp</code>, <code>mkdir</code>) on the login nodes. The 
execution of scripts, including cron, software, and software compilation on the login nodes
is prohibited (and could VERY likely crash the head node). However, users may run small scripts 
and commands (to compress data, create directories, etc.) that run within a few minutes, minimizing
their use as much as possible. If you are unsure if your scripts are suitable 
for running on the login nodes, consider using an interactive job or contact us at <a href="mailto:chtc@cs.wisc.edu">chtc@cs.wisc.edu</a>.</p>

<p><strong>CHTC staff reserve the right to kill any long-running or problematic processes on the 
head nodes and/or disable user accounts that violate this policy</strong></p>

<p><strong>2. The HPC Cluster is Reserved for MPI-enabled, Multi-node Jobs</strong> <br />
HPC users should not run single-core or single-node jobs to the HPC Cluster, given its 
optimization for multi-node/MPI-based work. Users will 
be asked to transition work appropriately to our high-throughput computing system.</p>

<p><strong>3. Maintain Copies of Essential Data in non-CHTC Locations</strong>  <br />
The HPC Cluster filesystem should be treated as temporary/scratch space, and only files necessary for 
actively running jobs should be kept on the filesystem. Once your jobs complete, 
your files should be removed from the cluster filesystem. Campus researchers have several options 
for persistent data storage solutions, including <a href="https://it.wisc.edu/services/researchdrive/">ResearchDrive</a> 
which provides up to 5TB of storage for free per research PI. Our guide 
<a href="transfer-data-researchdrive.shtml">Transferring Files Between CHTC and ResearchDrive</a> provides 
step-by-step instructions for transferring your data between HPC Cluster and ResearchDrive.</p>

<p>CHTC Staff reserve the right to remove any significant amounts of data on the HPC Cluster 
in our efforts to maintain filesystem performance for all users, though we will always 
first ask users to remove excess data and minimize file counts before taking additional action.</p>

<p><strong>4. Fair-share Policy</strong><br />
To promote fair access to HPC computing resources, all users are limited to 10 concurrently 
running jobs at a time. Additionally, user are restricted to a total of 600 cores 
across all running jobs. Core limits do not apply on research group partitions of
more than 600 cores.</p>

<p><strong>5. Job Priority Policy</strong></p>

<p>A. User priority decreases as the user accumulates hours of CPU time over the last 21 days, across 
all queues. This “fair-share” policy means that users who have run many/larger jobs in the near-past 
will have a lower priority, and users with little recent activity will see their waiting jobs start sooner. We do 
NOT have a strict “first-in-first-out” queue policy.</p>

<p>B. Job priority increases with job wait time. After the history-based user priority calculation in (A), 
the next most important factor for each job’s priority is the amount of time that each job has already 
waited in the queue. For all the jobs of a single user, these jobs will most closely follow a “first-in-first-out” policy.</p>

<p>C. Job priority increases with job size, in cores. This least important factor slightly favors larger jobs, as a means of 
somewhat countering the inherently longer wait time necessary for allocating more cores to a single job.</p>

<h1 id="hpc-hardware-and-configuration">HPC Hardware and Configuration</h1>

<p>The HPC Cluster consists of two login nodes and many compute (aka execute) 
nodes. All users log in at a login node, and all user files
on the shared file sytem are accessible on all nodes.
Additionally, all nodes are tightly networked (56 Gbit/s Infiniband) so
they can work together as a single "supercomputer", depending on the
number of CPUs you specify.</p>

<h2 id="login-nodes">Login Nodes</h2>

<p>The two login nodes for the cluster are:</p>
<ul>
  <li><code>hpclogin1.chtc.wisc.edu</code></li>
  <li><code>hpclogin2.chtc.wisc.edu</code></li>
</ul>

<p>For more details on logging in, see the “Connecting to CHTC” guide linked above.</p>

<h2 id="execute-nodes">Execute Nodes</h2>

<p>Only execute nodes will be used for performing your computational work. 
The execute nodes are organized into several "partitions", including 
the <code>univ2</code>, <code>pre</code>, and <code>int</code> partitions which are available to 
all HPC users as well as research group specific partitions that consist 
of researcher owned hardware and which all HPC users can access on a 
backfill capacity via the <code>pre</code> partition (more details below).</p>

<h2 id="partitions">Partitions</h2>

<table class="gtable">
  <thead>
    <tr>
      <th>Partition</th>
      <th>p-name</th>
      <th># nodes (N)</th>
      <th>t-max</th>
      <th>t-default</th>
      <th>max nodes/job</th>
      <th>cores/node (n)</th>
      <th>RAM/node (GB)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>University 2</td>
      <td>univ2</td>
      <td>148</td>
      <td>7 days</td>
      <td>1 day</td>
      <td>16</td>
      <td>20</td>
      <td>128</td>
    </tr>
    <tr>
      <td>Interactive</td>
      <td>int</td>
      <td>6</td>
      <td>1 hr</td>
      <td>1 hr</td>
      <td>1</td>
      <td>20</td>
      <td>128</td>
    </tr>
    <tr>
      <td>Pre-emptable (backfill)</td>
      <td>pre</td>
      <td>316</td>
      <td>24 hrs</td>
      <td>4 hrs</td>
      <td>16</td>
      <td>20</td>
      <td>128</td>
    </tr>
    <tr>
      <td>Owners</td>
      <td><em>unique</em></td>
      <td>124</td>
      <td>7 days</td>
      <td>24 hrs</td>
      <td><em>unique</em></td>
      <td>20</td>
      <td>128</td>
    </tr>
    <tr>
      <td>Astronomy Dept (differs)</td>
      <td>astro3</td>
      <td>24</td>
      <td><em>4 days</em></td>
      <td>24 hrs</td>
      <td>16</td>
      <td>20</td>
      <td>128</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>
    <p><code>univ2</code> consists of our second generation compute nodes, each with 20 
CPU cores of 2.5 GHz and 128 GB of RAM. Like <code>univ</code>, jobs submitted to this partition 
will not be pre-empted and can run for up to 7 days.</p>
  </li>
  <li>
    <p><code>int</code> consists of two compute nodes is intended for short and immediate interactive 
testing on a single node (up to 16 CPUs, 64 GB RAM). Jobs submitted to this partition 
can run for up to 1 hour.</p>
  </li>
  <li>
    <p><code>pre</code> (i.e. pre-emptable) is an under-layed partition encompassing all HPC compute 
nodes. This partiton is intended for more immediate turn-around of shorter and somewhat 
smaller jobs, or for interactive sessions requiring more than the 30-minute limit of 
the <code>int</code> partition. Jobs submitted to <code>pre</code> are pre-emptable and can run for up to 24 
hours. <code>pre</code> partition jobs will run on any idle nodes, including researcher owned 
compute nodes nodes, as back-fill meaning these jobs may be pre-empted by higher priority 
jobs. However, pre-empted jobs will be re-queued when submitted with an sbatch script.</p>
  </li>
</ul>

<h2 id="operating-system-and-software">Operating System and Software</h2>

<p>All nodes in the HPC Cluster are running CentOS 7 Linux.</p>

<p>The SLURM scheduler version is 20.02.2.</p>

<p>To see more details of other software on the cluster, see the <a href="/hpc-software">HPC Software page</a>.</p>

<h1 id="data-storage-and-management">Data Storage and Management</h1>

<p><strong>Data space in the HPC Cluster filesystem is not backed-up and should be
treated as temporary by users</strong>. Only files necessary for
<em>actively-running</em> jobs should be kept on the filesystem, and files
should be removed from the cluster when jobs complete. A copy of any
essential files should be kept in an alternate, non-CHTC storage
location.</p>

<p>Each user will receive two primary data storage locations:</p>

<ol>
  <li>
    <p><code>/home/username</code> with an initial disk quota of 100GB 
and 10,000 items. With the exception of software, all of the files 
needed for your work, such as input, output, configuration files, etc. 
should be located in your <code>/home</code> directory.</p>
  </li>
  <li>
    <p><code>/software/username</code> with an initial disk quota of 10GB and 
100,000 items. All software, library, etc. installtions should 
be written to and located in your <code>/software</code> directory.</p>
  </li>
</ol>

<p>To check how many files and directories you have in
your <code>/home</code> or <code>/software</code> directory see the 
<a href="#tools-for-managing-home-and-software-space">instructions below</a>.</p>

<p>Increased quotas to either of these locations are available upon email 
request to <a href="mailto:chtc@cs.wisc.edu">chtc@cs.wisc.edu</a>. In your request, 
please include both size (in GB) and file/directory counts. If you don't 
know how many files your installation creates, because it's more than 
the current items quota, simply indicate that in your request.</p>

<p><strong>CHTC Staff reserve the right to remove any significant amounts of data
on the HPC Cluster</strong> in our efforts to maintain filesystem performance
for all users, though we will always first ask users to remove excess
data and minimize file counts before taking additional action.</p>

<p><strong>Local scratch space</strong> of 500 GB is available on each execute node in
<code>/scratch/local/$USER</code> and is automatically cleaned out upon completion
of scheduled job sessions (interactive or non-interactive). Local
scratch is available on the login nodes, <code>hpclogin1</code> and <code>hpclogin2</code>, also at 
<code>/scratch/local/$USER</code> and should be cleaned out by the user upon completion of
compiling activities. CHTC staff will otherwise clean this location of
the oldest files when it reaches 80% capacity.</p>

<h2 id="tools-for-managing-home-and-software-space">Tools for managing home and software space</h2>

<p>You can use the command <code>get_quotas</code> to see what disk 
and items quotas are currently set for a given directory path. 
This command will also let you see how much disk is in use and how many 
items are present in a directory:</p>

<pre class="term"><code>[username@hpclogin1 ~]$ get_quotas /home/username /software/username
</code></pre>

<p>Alternatively, the <code>ncdu</code> command can also be used to see how many 
files and directories are contained in a given path:</p>

<pre class="term"><code>[username@hpclogin1 ~]$ ncdu /home/username
[username@hpclogin1 ~]$ ncdu /software/username
</code></pre>

<p>When <code>ncdu</code> has finished running, the output will give you a total file
count and allow you to navigate between subdirectories for even more
details. Type <code>q</code> when you're ready to exit the output viewer. More
info here: <a href="https://lintut.com/ncdu-check-disk-usage/">https://lintut.com/ncdu-check-disk-usage/</a></p>



<div id = "copyright">
     <p align = "center"> 
	  
     For all user support, questions, and comments:
     <strong><a href="mailto:chtc@cs.wisc.edu">chtc@cs.wisc.edu</a></strong>

</p>
</div>

</div>

  </td>
  </tr>

</TABLE>
</td>
</table>

</body>
</html>


